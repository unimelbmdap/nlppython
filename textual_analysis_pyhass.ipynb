{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjT7ZEogEfEY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning outcomes\n",
        "\n",
        "- Understand how and why to preprocess and basic preprocessing principles\n",
        "\n",
        "- Implement some forms of automated content analysis\n",
        "\n",
        "- Understand relational analysis in textual data with network analysis\n",
        "\n",
        "- Analyse  subjective meaning, not just literal content, in language such as sentiment in language"
      ],
      "metadata": {
        "id": "iNGTfJ2kxjqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with NLTK\n",
        "\n",
        "The Natural Language Toolkit, or [NLTK](https://www.nltk.org/), is a Python library for working with natural (human) language data in a computational way. It is mainly used for natural language processing (NLP), which entails analysing and modeling language with algorithms.\n",
        "\n",
        "Developed in the early 2000s by computational linguists Steven Bird and Edward Loper at the University of Pennsylvania, it was created as a teaching and research tool for linguistics and computer science students.\n",
        "\n",
        "NLTK has since become a widely used toolkit, providing tools for tokenization, tagging, parsing, and accessing standard linguistic datasets, making it especially useful for learning and research.\n"
      ],
      "metadata": {
        "id": "r1bHNWN6EiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "cra1vRmjEwGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "7rHuDKKkEjje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"This is an example sentence to demonstate basic NLP preprocessing steps.\"\n",
        "text"
      ],
      "metadata": {
        "id": "bYpL9ecME0_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# lowercasing\n",
        "text_lower = text.lower()\n",
        "text_lower\n",
        "# why lowercasing? treats 'The' and 'the' as the same word, for e.g."
      ],
      "metadata": {
        "id": "8nPJRQLnE9dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenisation\n",
        "tokens = word_tokenize(text_lower)\n",
        "tokens"
      ],
      "metadata": {
        "id": "eZelTdffFMC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokens[2]"
      ],
      "metadata": {
        "id": "KdjQYMnzHDkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7307c29"
      },
      "source": [
        "# removing punctuation (optional, but often done)\n",
        "# we'll use list comprehension to keep only alphabetic tokens\n",
        "tokens_no_punct = [word for word in tokens if word.isalpha()]\n",
        "tokens_no_punct"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# new list without non alpha tokens\n",
        "# this is the same as the list comprehension but\n",
        "# you might be more familiar with this for loop\n",
        "tokens_no_punct = []\n",
        "for word in tokens:\n",
        "    if word.isalpha(): # conditional\n",
        "        tokens_no_punct.append(word) # list method to append to list\n",
        "tokens_no_punct\n",
        "\n",
        "# list comprehension is more concise for our purposes"
      ],
      "metadata": {
        "id": "utA2NdCNH6Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# removing stopwords\n",
        "stop_words = stopwords.words('english') # variable from nltk. It's a list.\n",
        "stop_words.extend([\"would\", \"could\", \"said\", \"must\",\n",
        "                       \"much\", \"miss\", \"one\"])  # Add custom stop words\n",
        "tokens_no_stopwords = [word for word in tokens_no_punct if word not in stop_words]\n",
        "# another list comprehension to remove \"stopwords\", but in this case\n",
        "# if NOT IN\n",
        "tokens_no_stopwords\n"
      ],
      "metadata": {
        "id": "UUug6i4iIJOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why remove stopwords? Words like 'the', 'a', 'is' are very common in English but don't have much specific meaning, removing them can reduce noise."
      ],
      "metadata": {
        "id": "YqMj7rF1IfD2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lemmatizer = WordNetLemmatizer()\n",
        "lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n",
        "lemmatized_tokens\n"
      ],
      "metadata": {
        "id": "xwoeI56lIhkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization produces dictionary base words, which can be useful for tasks requiring semantic understanding. It allows the code to interpret different variations of the same word e.g. run, ran or politician, politicians as the same meaning. This can be very useful if trying to understand what a text is about or what is talked about. If you are studying linguistics, these difference may be important to keep and therefore you would not lemmatise. Preprocessing depends on your research domain and research goals."
      ],
      "metadata": {
        "id": "x63EWSrCJ376"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.pos_tag(tokens_no_stopwords, tagset='universal')"
      ],
      "metadata": {
        "id": "IVnlKZCmIK6j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Tag  | Meaning                   | English Examples                                      |\n",
        "|------|---------------------------|--------------------------------------------------------|\n",
        "| ADJ  | adjective                 | new, good, high, special, big, local                   |\n",
        "| ADP  | adposition                | on, of, at, with, by, into, under                      |\n",
        "| ADV  | adverb                    | really, already, still, early, now                     |\n",
        "| CONJ | conjunction               | and, or, but, if, while, although                      |\n",
        "| DET  | determiner, article       | the, a, some, most, every, no, which                  |\n",
        "| NOUN | noun                      | year, home, costs, time, Africa                        |\n",
        "| NUM  | numeral                   | twenty-four, fourth, 1991, 14:24                       |\n",
        "| PRT  | particle                  | at, on, out, over, per, that, up, with                |\n",
        "| PRON | pronoun                   | he, their, her, its, my, I, us                         |\n",
        "| VERB | verb                      | is, say, told, given, playing, would                  |\n",
        "| .    | punctuation marks         | . , ; !                                               |\n",
        "| X    | other                     | ersatz, esprit, dunno, gr8, univeristy                |\n",
        "\n",
        "from https://www.nltk.org/book/ch05.html 2.3 \"A Universal Part-of-Speech Tagset\" Table 2.1:"
      ],
      "metadata": {
        "id": "0YJFP4PbNFl7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# it is very common to create a preprocessing function\n",
        "# code that can be reused. Here we put is all together\n",
        "\n",
        "def process(text):\n",
        "    text_lower = text.lower()\n",
        "    tokens = word_tokenize(text_lower)\n",
        "    tokens_no_punct = [word for word in tokens if word.isalpha()]\n",
        "    tokens_no_stopwords = [word for word in tokens_no_punct if word not in stop_words]\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens_no_stopwords]\n",
        "    return lemmatized_tokens"
      ],
      "metadata": {
        "id": "JF4BXVfXN4FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "![image.png](attachment:image.png)"
      ],
      "metadata": {
        "id": "-slohJ7gPT8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Analysis\n",
        "Content analysis has a long tradition in the humanities and social sciences, with seminal works by Krippendorff and others. Traditional quantitative textual analysis techniques often necessitated large teams manually counting words by hand and applying elaborate \"coding frameworks\" (not computer code, but rules for interpretation). Researchers today increasingly use automated content analysis methods using NLP methods, which can tokenize and count words efficiently at scale. We will cover several approaches to counting words, ranging from simple but powerful frequency distributions to more rigorous statistical techniques such as topic modeling."
      ],
      "metadata": {
        "id": "eKvCRgrfVNi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import requests\n",
        "from nltk import FreqDist"
      ],
      "metadata": {
        "id": "3uP9MYD9ZVOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Gutenberg\n",
        "[Project Gutenberg](https://www.gutenberg.org/) is a free online library that offers over 60,000 eBooks, mostly classic literature. It makes these texts freely available to the public in digital formats like plain text, which is very useful for NLP."
      ],
      "metadata": {
        "id": "e2MQUsgyX8a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load books\n",
        "urls = {\n",
        "    \"Emma\": \"https://www.gutenberg.org/files/158/158-0.txt\",\n",
        "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"Sense and Sensibility\": \"https://www.gutenberg.org/files/161/161-0.txt\",\n",
        "    \"Mansfield Park\": \"https://www.gutenberg.org/files/141/141-0.txt\",\n",
        "}"
      ],
      "metadata": {
        "id": "XwBcFMShX50p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Process each book as a dictionary\n",
        "books = {}\n",
        "for title, url in urls.items():\n",
        "    # urls.items() is a method used on a Python dictionary called urls.\n",
        "    # it allows you to loop over both keys and values at the same time\n",
        "    response = requests.get(url)\n",
        "    # requests is a Python library used to make HTTP requests,\n",
        "    # like visiting a web page or downloading a file from the internet.\n",
        "\n",
        "    raw_text = response.text\n",
        "    # extracts the text content of the response\n",
        "\n",
        "    # cleanup to remove Gutenberg header/footer\n",
        "    lines = raw_text.splitlines()\n",
        "    # print(lines[:10]) TO SHOW WHAT IT IS DOING\n",
        "\n",
        "    start = 0\n",
        "    end = len(lines)\n",
        "    # initialise the start and end line index\n",
        "\n",
        "    for i, line in enumerate(lines):\n",
        "    # IF CONFUSED COULD PRINT WITH TOKENS\n",
        "      # loops through each line with its index i, which is what\n",
        "      # enumerate does\n",
        "        lower_line = line.lower()\n",
        "\n",
        "        if '*** start of the project gutenberg ebook' in lower_line:\n",
        "            start = i + 1\n",
        "\n",
        "        elif '*** end of the project gutenberg ebook' in lower_line:\n",
        "            end = i\n",
        "            break\n",
        "\n",
        "\n",
        "    clean_text = '\\n'.join(lines[start:end]).strip()\n",
        "\n",
        "    text = clean_text.lower()\n",
        "\n",
        "    words = process(text)\n",
        "\n",
        "    books[title] = {\n",
        "        \"text\": text,\n",
        "        \"words\": words\n",
        "    }\n",
        "\n",
        "    # frequency and plot\n",
        "    freq = FreqDist(words)\n",
        "    top_words = freq.most_common(10)\n",
        "    words_, counts = zip(*top_words)\n",
        "    # adding an underscore (like words_) is a convention often\n",
        "    #used to avoid naming conflicts\n",
        "    # *top_words the * unpacks the list\n",
        "\n",
        "    plt.figure(figsize=(6, 3))\n",
        "    plt.bar(words_, counts, color='skyblue')\n",
        "    plt.title(f\"Top Words in '{title}'\")\n",
        "    # using an f-string (formatted string literal) to dynamically\n",
        "    # insert a variable into a string i.e. title of each book\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "id": "nTzBeZpPBlME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling"
      ],
      "metadata": {
        "id": "s-AR1mnguT1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n"
      ],
      "metadata": {
        "id": "6jN72x0qbi2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation, or LDA, is a technique to uncover the hidden topics that appear across a collection of texts. Instead of just counting how often words appear, LDA looks for patterns using probability, which words tend to appear together etc, and then groups them into meaningful topics. Each document (text) is  seen as a mix of these topics and may be label with a dominant topic.\n",
        "\n",
        "This can be more powerful than word counting because it captures the themes or ideas running through the text. It helps reveal what the text is about, even if certain key words are used in different ways across documents."
      ],
      "metadata": {
        "id": "IVRhZvvtYJuD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare documents as strings (joining tokens back)\n",
        "docs = [\n",
        "    ' '.join(book[\"words\"][i:i+500])\n",
        "    for book in books.values()\n",
        "    # .values() is a dictionary method that returns a\n",
        "    # special view of all the values in the dictionary\n",
        "    for i in range(0, len(book[\"words\"]), 500)\n",
        "    if len(book[\"words\"][i:i+500]) > 50\n",
        "    # this is list slicing i to i + 500 tokens, chuck size\n",
        "]\n",
        "# this string method takes a list of strings and joins them into a single string"
      ],
      "metadata": {
        "id": "C3lDMa0obWAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(docs)"
      ],
      "metadata": {
        "id": "g3VT5T82YUfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create CountVectorizer to convert text to term-frequency matrix\n",
        "vectorizer = CountVectorizer()\n",
        "X = vectorizer.fit_transform(docs)\n",
        "# converts a list of text documents into a matrix of word counts\n",
        "# also known as a document-term matrix (DTM)."
      ],
      "metadata": {
        "id": "9iu5QTGQcGNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of what a Term-Frequency Matrix (X) might look like:\n",
        "\n",
        "| Word  | victim | walk | love | cry | darcy | bennet | thought | may |\n",
        "| ----- | --- | --- | -- | --- | --- | --- | --- | ------ |\n",
        "| Doc 1 | 1   | 1   | 1  | 2   | 1   | 0   | 0   | 0      |\n",
        "| Doc 2 | 0   | 1   | 1  | 2   | 0   | 1   | 1   | 0      |\n",
        "| Doc 3 | 1   | 0   | 0  | 2   | 0   | 1   | 0   | 1      |\n",
        "\n",
        "Each document is now a vector of numbers representing word frequency.\n"
      ],
      "metadata": {
        "id": "aLPzze9vpeW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the LDA model to uncover hidden topics in a set of documents. It’s set to group by 15 topics, but that is arbitary at this stage. The model looks at how words are distributed across the documents and groups them into topics based on the patterns it finds. lda.fit(X) applies the model to the term-frequency matrix (X) and \"learns\" what topics are present.\n",
        "\n",
        "There are techniques to help determine the best number of topics in documents, including topic coherence scores, as well as topic models that infer the number of topics."
      ],
      "metadata": {
        "id": "AlSc6RkwrUDG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# fit LDA model, e.g. 15 topics\n",
        "lda = LatentDirichletAllocation(n_components=15, random_state=42)\n",
        "lda.fit(X)"
      ],
      "metadata": {
        "id": "xY0oX2GGcLfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names (words)\n",
        "words = vectorizer.get_feature_names_out()\n",
        "# get the list of all the unique words (features) the vectorizer learned"
      ],
      "metadata": {
        "id": "xlKTKowacRrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can imagine LDA components like this:\n",
        "\n",
        "| Topic ↓ / Word Index → | 0    | 1    | 2    | 3    | 4    |\n",
        "| ---------------------- | ---- | ---- | ---- | ---- | ---- |\n",
        "| **Topic 0**            | 0.01 | 0.09 | 0.12 | 0.04 | 0.03 |\n",
        "| **Topic 1**            | 0.03 | 0.02 | 0.01 | 0.12 | 0.08 |\n"
      ],
      "metadata": {
        "id": "Z4tSuNuVzm-M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# display top words per topic\n",
        "for i, topic in enumerate(lda.components_):\n",
        "  # lda.components a 2D array (matrix) from your trained LDA model\n",
        "  # each row is a topic and each number in the row is the importance (weight)\n",
        "  # of a word for that topic.\n",
        "\n",
        "    top_indices = topic.argsort()[-5:][::-1]\n",
        "    # returns the indices of words sorted by their weight gets the top 5\n",
        "    top_words = [words[j] for j in top_indices]\n",
        "    # a list of vocab words from get_feature_names_out() above\n",
        "    print(f\"Topic {i+1}: {', '.join(top_words)}\")"
      ],
      "metadata": {
        "id": "ayL4Ypg3cTPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can imagine words like this:\n",
        "\n",
        "| **Column Index** →  | **Word** |\n",
        "| ---------------- | -------- |\n",
        "| 0                | every    |\n",
        "| 1                | walk     |\n",
        "| 2                | mother   |\n",
        "| 3                | sister   |\n",
        "| 4                | fanny    |\n",
        "\n",
        "So this is why we need to map the terms from words back to the components that have the weights of words in topics."
      ],
      "metadata": {
        "id": "DhM_xPjZzzUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Analysis of Birgrams"
      ],
      "metadata": {
        "id": "fgJeoLHk1n0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "rrbdZsAigHfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams are pairs of two words (could be more, trigrams for e.g.) that appear next to each other in a sentence. Bigrams often carry more meaning together then studying single words alone. For example, the words “New” and “York” separately aren't as meaningful as the bigram “New York,” which clearly refers to a place. Using bigrams helps capture these kinds of relationships between words, which can make text analysis or language models more accurate."
      ],
      "metadata": {
        "id": "MYIbRQADr4ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# collect bigrams from each book’s tokenised words\n",
        "bigram_counts = Counter()\n",
        "# Counter() is a special class from Python to count things.\n",
        "\n",
        "for title, book_data in books.items():\n",
        "    words = book_data[\"words\"]\n",
        "    bigrams = list(ngrams(words, 2))\n",
        "    bigram_counts.update(bigrams)"
      ],
      "metadata": {
        "id": "epmlfVQOgKqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# get top 50 most common bigrams\n",
        "top_bigrams = bigram_counts.most_common(50)\n",
        "top_bigrams[0:10]"
      ],
      "metadata": {
        "id": "7-sslUdugOfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# build network\n",
        "G = nx.Graph()\n",
        "# creates an empty, undirected graph\n",
        "for (w1, w2), count in top_bigrams:\n",
        "  #loop iterates over each bigram (word1 and word2) and its count\n",
        "    G.add_edge(w1, w2, weight=count)"
      ],
      "metadata": {
        "id": "PpQ_gzi5gb-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# layout\n",
        "pos = nx.spring_layout(G, k=0.8)\n",
        "\n",
        "plt.figure(figsize=(9, 7))\n",
        "nx.draw_networkx(\n",
        "    G, pos,\n",
        "    with_labels=True,\n",
        "    node_color='skyblue',\n",
        "    edge_color='gray',\n",
        "    node_size=300,\n",
        "    font_size=10,\n",
        "    font_color='black'\n",
        ")\n",
        "plt.title('Bigram Network')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "X4SC_XLSRAYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis\n",
        "\n",
        "Sentiment analysis in NLP can be approached in a number of ways. Three importance approaches are: lexicon-based methods, rule-based systems, and machine learning models.\n",
        "\n",
        "Lexicon-based methods use predefined sentiment dictionaries to associate each word with polarity or score. Sentiment is derived from aggregating these scores. This technique does not require training data.\n",
        "\n",
        "Rule-based systems extend the lexicon approach by incorporating linguistic rules such as handling negations or intensifiers, for example.\n",
        "\n",
        "Machine learning treats sentiment analysis as a supervised classification problem, meaning that models learn patterns from labeled data and then can (hopefully) generalise what they have learned to new examples. Models have included Naive Bayes, logistic regression, and support vector machines, but more recent techniques often draw on [deep learning](https://en.wikipedia.org/wiki/Deep_learning), particularly transformer-based models, which capture more word context."
      ],
      "metadata": {
        "id": "eyOTGRPCOGN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "# download VADER lexicon\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "k3U-e2hfOIPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have been doing analysis at the word level, but we can do analysis at the sentence level as well. To do this we need to tokenise the data by sentence."
      ],
      "metadata": {
        "id": "7rn3uyAEoBsg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "for title, book_data in books.items():\n",
        "    text = book_data[\"text\"]  # full book text\n",
        "    sentences = sent_tokenize(text)  # split into sentences\n",
        "    book_data[\"sentences\"] = sentences  # add to the book's data\n"
      ],
      "metadata": {
        "id": "sBtei-blnwWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['they seemed more like cheerful, easy friends, than lovers.',\n",
        " 'how could she have been so brutal, so cruel to miss bates!',\n",
        " 'mrs. collins welcomed her friend with the liveliest pleasure, and elizabeth was more and more satisfied with coming, when she found herself so affectionately received.',\n",
        " 'kitty was the only one who shed tears; but she did weep from vexation and envy.',\n",
        " 'to her it was but the natural consequence of a strong affection in a young and ardent mind.',\n",
        " 'by a former marriage, mr. henry dashwood had one son: by his present lady, three daughters.',\n",
        " 'but the feelings which made such composure a disgrace, left her in no danger of incurring it.',\n",
        " 'she had been a beauty, and a prosperous beauty, all her life; and beauty and wealth were all that excited her respect.',\n",
        " 'she could hardly have made a more untoward choice.',\n",
        " 'but her uncle’s anger gave her the severest pain of all.']"
      ],
      "metadata": {
        "id": "TvcNnL7Ciif8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vader_analyzer = SentimentIntensityAnalyzer()\n",
        "#  creates an instance of the SentimentIntensityAnalyzer class from the\n",
        "# VADER sentiment analysis tool.\n",
        "# When you create vader_analyzer, you get an object ready to analyze text\n",
        "for sentence in sentences:\n",
        "    scores = vader_analyzer.polarity_scores(sentence)\n",
        "    print(f\"Sentence: {sentence}\\n Score: {scores['compound']}\")"
      ],
      "metadata": {
        "id": "KtnIXiIfOO8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hugging Face](https://huggingface.co/) is an open-source platform that provides a high-level interface for some very powerful NLP models. Through its transformer library it simplies tradtional NLP pipelines, including much of what we have been doing \"by hand\" e.g. tokenising etc.."
      ],
      "metadata": {
        "id": "eOPizWam5PFm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hugging Face transformer Sentiment Analysis\n",
        "from transformers import pipeline\n",
        "# load default model (distilbert fine-tuned on sentiment)\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "# pipeline is a function imported from the transformers library by Hugging Face.\n",
        "for sentence in sentences:\n",
        "    result = classifier(sentence)[0]\n",
        "    print(f\"Sentence: {sentence}\\nLabel: {result['label']}, Score: {result['score']}\\n\")"
      ],
      "metadata": {
        "id": "ZmEvuJfjOaGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which sentiment scores do you agree with more? Are the scores accurate in your expert human opinion?"
      ],
      "metadata": {
        "id": "yfqrfb0J6ELY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Understand how and why to preprocess and basic preprocessing principles**\n",
        "\n",
        "- Learned important principles in nlp such as tokenisation, stop words and lemmatisation.\n",
        "\n",
        "- We have learn how to preprocess texts for different types of analysis, by word and by sentence.\n",
        "\n",
        "- We have formatted that textual data as appropriate for the analysis tool: as string, tokens, bigrams and chunks of text.\n",
        "\n",
        "**2. Implement some forms of automated content analysis**\n",
        "\n",
        "- We have created frequency distributions to count words across a corpora of Austin's works\n",
        "\n",
        "- Used one technique of breaking the text into a meaningful size for content analysis (we could also have chosen paragraphs, chapters or books if appropriate).\n",
        "\n",
        "**3. Understand relational analysis in textual data with network analysis**\n",
        "- Learn how to create and work with bigrams\n",
        "- Created a graph representation of words that can also be applied to other entities in text\n",
        "\n",
        "**4. Analyse subjective meaning, not just literal content, in language such as sentiment in language**\n",
        "\n",
        "- Prepared and compared textual data for subjective meaning and compared the results of different computational methods of sentiment analysis, including a largely lexicon and rule-based sentiment analyser like VADER and a more advanced transformer-based encoder model (DistilBERT) that processes text as a sequence, capturing the meaning of each word based on its surrounding words (context).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1x6ToepkfVKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FURTHER RESOURCES\n",
        "\n",
        "The [NLTK book](https://www.nltk.org/book/) is available for free online and is an accessable pathway to get a handle on the basics of NLP.\n",
        "\n",
        "[Real Python course](https://realpython.com/nltk-nlp-python/) on NLP will help cement the basics.\n",
        "\n",
        "An advanced free course with spaCy: https://course.spacy.io/en/"
      ],
      "metadata": {
        "id": "Qw9JzAMvjgke"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWCn1JcFjeDX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}