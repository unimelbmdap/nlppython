{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjT7ZEogEfEY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Learning outcomes\n",
        "\n",
        "- Understand how and why to preprocess and basic preprocessing principles\n",
        "\n",
        "- Implement some forms of automated content analysis\n",
        "\n",
        "- Understand relational analysis in textual data with network analysis\n",
        "\n",
        "- Analyse  subjective meaning, not just literal content, in language such as sentiment in language"
      ],
      "metadata": {
        "id": "iNGTfJ2kxjqM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing with NLTK\n",
        "\n",
        "The Natural Language Toolkit, or [NLTK](https://www.nltk.org/), is a Python library for working with natural (human) language data in a computational way. It is mainly used for natural language processing (NLP), which entails analysing and modeling language with algorithms.\n",
        "\n",
        "Developed in the early 2000s by computational linguists Steven Bird and Edward Loper at the University of Pennsylvania, it was created as a teaching and research tool for linguistics and computer science students.\n",
        "\n",
        "NLTK has since become a widely used toolkit, providing tools for tokenization, tagging, parsing, and accessing standard linguistic datasets, making it especially useful for learning and research.\n"
      ],
      "metadata": {
        "id": "r1bHNWN6EiRo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ],
      "metadata": {
        "id": "cra1vRmjEwGd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import packages\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer"
      ],
      "metadata": {
        "id": "7rHuDKKkEjje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYpL9ecME0_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8nPJRQLnE9dK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eZelTdffFMC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KdjQYMnzHDkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c7307c29"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "utA2NdCNH6Vn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UUug6i4iIJOm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why remove stopwords? Words like 'the', 'a', 'is' are very common in English but don't have much specific meaning, removing them can reduce noise."
      ],
      "metadata": {
        "id": "YqMj7rF1IfD2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xwoeI56lIhkL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Why lemmatization? lemmatization produces actual dictionary words, which can be useful for tasks requiring semantic understanding. It allows the code to interpret different variations of the same word e.g. run, ran or politician, politicians as the same meaning. This can be very useful if trying to understand what a text is about or what is talked about. If you are studying linguistics, these difference may be important to keep and therefore you would not lemmatise. Preprocessing depends on your research domain and research goals."
      ],
      "metadata": {
        "id": "x63EWSrCJ376"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "JF4BXVfXN4FQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-slohJ7gPT8E"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Content Analysis\n",
        "Content analysis has a long tradition in the humanities and social sciences, with seminal works by Krippendorff and others. Traditional quantitative textual analysis techniques often necessitated large teams manually counting words by hand and applying elaborate \"coding frameworks\" (not computer code, but rules for interpretation). Researchers today increasingly use automated content analysis methods using NLP methods, which can tokenize and count words efficiently at scale. We will cover several approaches to counting words, ranging from simple but powerful frequency distributions to more rigorous statistical techniques such as topic modeling."
      ],
      "metadata": {
        "id": "eKvCRgrfVNi7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set()\n",
        "import requests\n",
        "from nltk import FreqDist\n",
        "import requests"
      ],
      "metadata": {
        "id": "3uP9MYD9ZVOC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Project Gutenberg\n",
        "[Project Gutenberg](https://www.gutenberg.org/) is a free online library that offers over 60,000 eBooks, mostly classic literature. It makes these texts freely available to the public in digital formats like plain text, which is very useful for NLP.\n",
        "\n",
        "*note: some of these exercises were inspired by [Text Mining with R](https://www.tidytextmining.com/)*"
      ],
      "metadata": {
        "id": "e2MQUsgyX8a-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load books\n",
        "urls = {\n",
        "    \"Emma\": \"https://www.gutenberg.org/files/158/158-0.txt\",\n",
        "    \"Pride and Prejudice\": \"https://www.gutenberg.org/files/1342/1342-0.txt\",\n",
        "    \"Sense and Sensibility\": \"https://www.gutenberg.org/files/161/161-0.txt\",\n",
        "    \"Mansfield Park\": \"https://www.gutenberg.org/files/141/141-0.txt\",\n",
        "}"
      ],
      "metadata": {
        "id": "XwBcFMShX50p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nTzBeZpPBlME"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Topic Modelling"
      ],
      "metadata": {
        "id": "s-AR1mnguT1J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation\n"
      ],
      "metadata": {
        "id": "6jN72x0qbi2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Latent Dirichlet Allocation, or LDA, is a technique to uncover the hidden topics that appear across a collection of texts. Instead of just counting how often words appear, LDA looks for patterns using probability, which words tend to appear together etc, and then groups them into meaningful topics. Each document (text) is  seen as a mix of these topics and may be label with a dominant topic.\n",
        "\n",
        "This can be more powerful than word counting because it captures the themes or ideas running through the text. It helps reveal what the text is about, even if certain key words are used in different ways across documents."
      ],
      "metadata": {
        "id": "IVRhZvvtYJuD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C3lDMa0obWAF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g3VT5T82YUfb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9iu5QTGQcGNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Example of what a Term-Frequency Matrix (X) might look like:\n",
        "\n",
        "| Word  | victim | walk | love | cry | darcy | bennet | thought | may |\n",
        "| ----- | --- | --- | -- | --- | --- | --- | --- | ------ |\n",
        "| Doc 1 | 1   | 1   | 1  | 2   | 1   | 0   | 0   | 0      |\n",
        "| Doc 2 | 0   | 1   | 1  | 2   | 0   | 1   | 1   | 0      |\n",
        "| Doc 3 | 1   | 0   | 0  | 2   | 0   | 1   | 0   | 1      |\n",
        "\n",
        "Each document is now a vector of numbers representing word frequency.\n"
      ],
      "metadata": {
        "id": "aLPzze9vpeW4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains the LDA model to uncover hidden topics in a set of documents. It’s set to group by 15 topics, but that is arbitary at this stage. The model looks at how words are distributed across the documents and groups them into topics based on the patterns it finds. lda.fit(X) applies the model to the term-frequency matrix (X) and \"learns\" what topics are present.\n",
        "\n",
        "There are techniques to help determine the best number of topics in documents, including topic coherence scores, as well as topic models that infer the number of topics."
      ],
      "metadata": {
        "id": "AlSc6RkwrUDG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xY0oX2GGcLfp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xlKTKowacRrM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can imagine LDA components like this:\n",
        "\n",
        "| Topic ↓ / Word Index → | 0    | 1    | 2    | 3    | 4    |\n",
        "| ---------------------- | ---- | ---- | ---- | ---- | ---- |\n",
        "| **Topic 0**            | 0.01 | 0.09 | 0.12 | 0.04 | 0.03 |\n",
        "| **Topic 1**            | 0.03 | 0.02 | 0.01 | 0.12 | 0.08 |\n"
      ],
      "metadata": {
        "id": "Z4tSuNuVzm-M"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ayL4Ypg3cTPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Can imagine words like this:\n",
        "\n",
        "| **Column Index** →  | **Word** |\n",
        "| ---------------- | -------- |\n",
        "| 0                | every    |\n",
        "| 1                | walk     |\n",
        "| 2                | mother   |\n",
        "| 3                | sister   |\n",
        "| 4                | fanny    |\n",
        "\n",
        "So this is why we need to map the terms from words back to the components that have the weights of words in topics."
      ],
      "metadata": {
        "id": "DhM_xPjZzzUH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Network Analysis of Birgrams"
      ],
      "metadata": {
        "id": "fgJeoLHk1n0T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.util import ngrams\n",
        "from collections import Counter\n",
        "import networkx as nx"
      ],
      "metadata": {
        "id": "rrbdZsAigHfI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bigrams are pairs of two words (could be more, trigrams for e.g.) that appear next to each other in a sentence. Bigrams often carry more meaning together then studying single words alone. For example, the words “New” and “York” separately aren't as meaningful as the bigram “New York,” which clearly refers to a place. Using bigrams helps capture these kinds of relationships between words, which can make text analysis or language models more accurate."
      ],
      "metadata": {
        "id": "MYIbRQADr4ux"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "epmlfVQOgKqY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7-sslUdugOfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PpQ_gzi5gb-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "X4SC_XLSRAYk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Sentiment analysis\n",
        "\n",
        "Sentiment analysis in NLP can be approached in a number of ways. Three importance approaches are: lexicon-based methods, rule-based systems, and machine learning models.\n",
        "\n",
        "Lexicon-based methods use predefined sentiment dictionaries to associate each word with polarity or score. Sentiment is derived from aggregating these scores. This technique does not require training data.\n",
        "\n",
        "Rule-based systems extend the lexicon approach by incorporating linguistic rules such as handling negations or intensifiers, for example.\n",
        "\n",
        "Machine learning treats sentiment analysis as a supervised classification problem, meaning that models learn patterns from labeled data and then can (hopefully) generalise what they have learned to new examples. Models have included Naive Bayes, logistic regression, and support vector machines, but more recent techniques often draw on [deep learning](https://en.wikipedia.org/wiki/Deep_learning), particularly transformer-based models, which capture more word context."
      ],
      "metadata": {
        "id": "eyOTGRPCOGN1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import nltk\n",
        "# download VADER lexicon\n",
        "nltk.download('vader_lexicon')"
      ],
      "metadata": {
        "id": "k3U-e2hfOIPD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So far we have been doing analysis at the word level, but we can do analysis at the sentence level as well. To do this we need to tokenise the data by sentence."
      ],
      "metadata": {
        "id": "7rn3uyAEoBsg"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sBtei-blnwWp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = ['they seemed more like cheerful, easy friends, than lovers.',\n",
        " 'how could she have been so brutal, so cruel to miss bates!',\n",
        " 'mrs. collins welcomed her friend with the liveliest pleasure, and elizabeth was more and more satisfied with coming, when she found herself so affectionately received.',\n",
        " 'kitty was the only one who shed tears; but she did weep from vexation and envy.',\n",
        " 'to her it was but the natural consequence of a strong affection in a young and ardent mind.',\n",
        " 'by a former marriage, mr. henry dashwood had one son: by his present lady, three daughters.',\n",
        " 'but the feelings which made such composure a disgrace, left her in no danger of incurring it.',\n",
        " 'she had been a beauty, and a prosperous beauty, all her life; and beauty and wealth were all that excited her respect.',\n",
        " 'she could hardly have made a more untoward choice.',\n",
        " 'but her uncle’s anger gave her the severest pain of all.']"
      ],
      "metadata": {
        "id": "TvcNnL7Ciif8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KtnIXiIfOO8i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Hugging Face](https://huggingface.co/) is an open-source platform that provides a high-level interface for some very powerful NLP models. Through its transformer library it simplies tradtional NLP pipelines, including much of what we have been doing \"by hand\" e.g. tokenising etc.."
      ],
      "metadata": {
        "id": "eOPizWam5PFm"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZmEvuJfjOaGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which sentiment scores do you agree with more? Are the scores accurate in your expert human opinion?"
      ],
      "metadata": {
        "id": "yfqrfb0J6ELY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1. Understand how and why to preprocess and basic preprocessing principles**\n",
        "\n",
        "- Learned important principles in nlp such as tokenisation, stop words and lemmatisation.\n",
        "\n",
        "- We have learn how to preprocess texts for different types of analysis, by word and by sentence.\n",
        "\n",
        "- We have formatted that textual data as appropriate for the analysis tool: as string, tokens, bigrams and chunks of text.\n",
        "\n",
        "**2. Implement some forms of automated content analysis**\n",
        "\n",
        "- We have created frequency distributions to count words across a corpora of Austin's works\n",
        "\n",
        "- Used one technique of breaking the text into a meaningful size for content analysis (we could also have chosen paragraphs, chapters or books if appropriate).\n",
        "\n",
        "**3. Understand relational analysis in textual data with network analysis**\n",
        "- Learn how to create and work with bigrams\n",
        "- Created a graph representation of words that can also be applied to other entities in text\n",
        "\n",
        "**4. Analyse subjective meaning, not just literal content, in language such as sentiment in language**\n",
        "\n",
        "- Prepared and compared textual data for subjective meaning and compared the results of different computational methods of sentiment analysis, including a largely lexicon and rule-based sentiment analyser like VADER and a more advanced transformer-based encoder model (DistilBERT) that processes text as a sequence, capturing the meaning of each word based on its surrounding words (context).\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "1x6ToepkfVKH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FURTHER RESOURCES\n",
        "\n",
        "The [NLTK book](https://www.nltk.org/book/) is available for free online and is an accessable pathway to get a handle on the basics of NLP.\n",
        "\n",
        "[Real Python course](https://realpython.com/nltk-nlp-python/) on NLP will help cement the basics.\n",
        "\n",
        "An advanced free course with spaCy: https://course.spacy.io/en/"
      ],
      "metadata": {
        "id": "Qw9JzAMvjgke"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "sWCn1JcFjeDX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}